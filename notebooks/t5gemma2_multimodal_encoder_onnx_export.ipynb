{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1ETwtgkmCDy00E2YNT0M4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IoT-gamer/t5gemma2-onnx/blob/main/notebooks/t5gemma2_multimodal_encoder_onnx_export.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T5gemma-2 Multimodal Encoder ONNX Export\n",
        "- exports only the encoder\n",
        "- encoder accepts text and/or image input\n",
        "- uses hugging face `transformers` and `torch.onnx.export`"
      ],
      "metadata": {
        "id": "WyncdhzXDssW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References/Acknowledgements\n",
        "- [T5Gemma 2 Hugging Face](https://huggingface.co/docs/transformers/model_doc/t5gemma2)\n",
        "- [Google Blog](https://blog.google/innovation-and-ai/technology/developers-tools/t5gemma-2/)"
      ],
      "metadata": {
        "id": "2aQ-arV5Al2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "-R_iNeQT5pK7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cihgr26LCozC"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade git+https://github.com/huggingface/transformers.git\n",
        "!pip install onnxscript"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model, Create Wrapper and Export"
      ],
      "metadata": {
        "id": "yzGv0rmKMqgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers.masking_utils\n",
        "from transformers import T5Gemma2ForConditionalGeneration\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# PATCHING\n",
        "# ---------------------------------------------------------\n",
        "# We patch the mask generation to avoid vmap/complex logic during tracing.\n",
        "def patched_create_bidirectional_mask(config, input_embeds, attention_mask, **kwargs):\n",
        "    batch_size, seq_length = input_embeds.shape[:2]\n",
        "    return torch.zeros((batch_size, 1, seq_length, seq_length), device=input_embeds.device)\n",
        "\n",
        "transformers.masking_utils.create_bidirectional_mask = patched_create_bidirectional_mask\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# WRAPPER\n",
        "# ---------------------------------------------------------\n",
        "class T5Gemma2EncoderMultimodalWrapper(nn.Module):\n",
        "    def __init__(self, encoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, pixel_values):\n",
        "        batch, seq = input_ids.shape\n",
        "\n",
        "        # Create the 4D attention mask [batch, 1, 1, seq] expanded to [batch, 1, seq, seq]\n",
        "        # This allows the ONNX model to handle text/image masking externally.\n",
        "        extended_mask = attention_mask.view(batch, 1, 1, seq).expand(batch, 1, seq, seq)\n",
        "        extended_mask = (1.0 - extended_mask.float()) * torch.finfo(torch.float32).min\n",
        "\n",
        "        # The encoder handles the vision tower projection and masked_scatter internally\n",
        "        outputs = self.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=extended_mask,\n",
        "            pixel_values=pixel_values,\n",
        "            return_dict=False\n",
        "        )\n",
        "        return outputs[0] # last_hidden_state\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# LOAD MODEL\n",
        "# ---------------------------------------------------------\n",
        "model_id = \"google/t5gemma-2-270m-270m\"\n",
        "print(f\"Loading {model_id}...\")\n",
        "model = T5Gemma2ForConditionalGeneration.from_pretrained(\n",
        "    model_id, torch_dtype=torch.float16, device_map=\"cpu\" # Use float16 to save 50% RAM\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "encoder = model.get_encoder()\n",
        "\n",
        "# Disable kernel wrappers if they exist to simplify the trace\n",
        "for layer in encoder.layers:\n",
        "    if hasattr(layer.self_attn.forward, \"__wrapped__\"):\n",
        "        layer.self_attn.forward = layer.self_attn.forward.__wrapped__\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# PREPARE DUMMY INPUTS\n",
        "# ---------------------------------------------------------\n",
        "# Dynamically pull constants from the config to prevent \"tensor a vs tensor b\" errors\n",
        "mm_tokens_per_image = model.config.encoder.mm_tokens_per_image\n",
        "image_token_index = model.config.encoder.image_token_index\n",
        "vision_cfg = model.config.encoder.vision_config\n",
        "\n",
        "batch = 1\n",
        "text_seq_len = 10\n",
        "# The total sequence must contain exactly mm_tokens_per_image instances of image_token_index\n",
        "total_seq_len = mm_tokens_per_image + text_seq_len\n",
        "\n",
        "# Build input_ids: [BOS, Image_Tokens..., Text_Tokens...]\n",
        "dummy_input_ids = torch.full((batch, total_seq_len), 1, dtype=torch.long)\n",
        "dummy_input_ids[0, 0] = 2 # BOS\n",
        "dummy_input_ids[0, 1 : 1 + mm_tokens_per_image] = image_token_index\n",
        "\n",
        "dummy_attention_mask = torch.ones((batch, total_seq_len), dtype=torch.long)\n",
        "\n",
        "# Create dummy pixel values based on vision config (standard Siglip is 224x224)\n",
        "dummy_pixel_values = torch.randn(\n",
        "    batch,\n",
        "    3,\n",
        "    vision_cfg.image_size,\n",
        "    vision_cfg.image_size\n",
        ")\n",
        "\n",
        "dummy_inputs = (dummy_input_ids, dummy_attention_mask, dummy_pixel_values)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# EXPORT\n",
        "# ---------------------------------------------------------\n",
        "print(\"Exporting Multimodal Encoder...\")\n",
        "try:\n",
        "    torch.onnx.export(\n",
        "        T5Gemma2EncoderMultimodalWrapper(encoder),\n",
        "        dummy_inputs,\n",
        "        \"t5gemma2_encoder_multimodal.onnx\",\n",
        "        opset_version=17, # Supports RoPE and newer vision ops\n",
        "        input_names=[\"input_ids\", \"attention_mask\", \"pixel_values\"],\n",
        "        output_names=[\"last_hidden_state\"],\n",
        "        dynamic_axes={\n",
        "            \"input_ids\": {0: \"batch\", 1: \"seq\"},\n",
        "            \"attention_mask\": {0: \"batch\", 1: \"seq\"},\n",
        "            \"pixel_values\": {0: \"batch\"},\n",
        "        },\n",
        "        dynamo=False\n",
        "    )\n",
        "    print(\"SUCCESS: The multimodal encoder has been exported.\")\n",
        "except Exception as e:\n",
        "    print(f\"Export failed: {e}\")"
      ],
      "metadata": {
        "id": "87BA-strEA1N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}