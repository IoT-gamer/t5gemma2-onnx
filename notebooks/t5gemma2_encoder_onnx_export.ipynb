{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZFftN3t++ISoPFx12MY3j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IoT-gamer/t5gemma2-onnx/blob/main/notebooks/t5gemma2_encoder_onnx_export.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T5gemma-2 Encoder ONNX Export\n",
        "- exports only the encoder\n",
        "- uses hugging face `trasnformers` and `torch.onnx.export`"
      ],
      "metadata": {
        "id": "WyncdhzXDssW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References/Acknowledgements\n",
        "- [T5Gemma 2 Hugging Face](https://huggingface.co/docs/transformers/model_doc/t5gemma2)\n",
        "- [Google Blog](https://blog.google/innovation-and-ai/technology/developers-tools/t5gemma-2/)"
      ],
      "metadata": {
        "id": "2aQ-arV5Al2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "-R_iNeQT5pK7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cihgr26LCozC"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade git+https://github.com/huggingface/transformers.git\n",
        "!pip install onnxscript"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model, Create Wrapper and Export"
      ],
      "metadata": {
        "id": "yzGv0rmKMqgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers.masking_utils\n",
        "from transformers import T5Gemma2ForConditionalGeneration\n",
        "\n",
        "# Patch\n",
        "def patched_create_bidirectional_mask(config, input_embeds, attention_mask, **kwargs):\n",
        "    # We return a simple zero-mask (no masking) to allow the tracer to pass.\n",
        "    # The actual masking logic will be handled by the attention_mask input in ONNX.\n",
        "    batch_size, seq_length = input_embeds.shape[:2]\n",
        "    return torch.zeros((batch_size, 1, seq_length, seq_length), device=input_embeds.device)\n",
        "\n",
        "# Apply the patch globally before the tracer starts\n",
        "transformers.masking_utils.create_bidirectional_mask = patched_create_bidirectional_mask\n",
        "\n",
        "# Load the model\n",
        "model_id = \"google/t5gemma-2-270m-270m\"\n",
        "model = T5Gemma2ForConditionalGeneration.from_pretrained(\n",
        "    model_id, torch_dtype=torch.float32, device_map=\"cpu\"\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "# Patch kernels to avoid hidden ops\n",
        "encoder = model.get_encoder()\n",
        "for layer in encoder.layers:\n",
        "    if hasattr(layer.self_attn.forward, \"__wrapped__\"):\n",
        "        layer.self_attn.forward = layer.self_attn.forward.__wrapped__\n",
        "\n",
        "class T5Gemma2EncoderWrapper(nn.Module):\n",
        "    def __init__(self, encoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Create a simplified 4D attention mask externally\n",
        "        # to avoid the model's internal vmap-based mask generation\n",
        "        batch, seq = input_ids.shape\n",
        "        # Standard 4D mask: [batch, 1, seq, seq]\n",
        "        extended_mask = attention_mask.view(batch, 1, 1, seq).expand(batch, 1, seq, seq)\n",
        "        extended_mask = (1.0 - extended_mask) * torch.finfo(torch.float32).min\n",
        "\n",
        "        return self.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=extended_mask, # Pass the pre-computed mask\n",
        "            return_dict=False\n",
        "        )[0]\n",
        "\n",
        "# Export with Legacy Tracer\n",
        "trace_len = 128\n",
        "dummy_inputs = (torch.ones((1, trace_len), dtype=torch.long), torch.ones((1, trace_len), dtype=torch.long))\n",
        "\n",
        "\n",
        "try:\n",
        "    torch.onnx.export(\n",
        "        T5Gemma2EncoderWrapper(encoder),\n",
        "        dummy_inputs,\n",
        "        \"t5gemma2_encoder.onnx\",\n",
        "        opset_version=17, # Supports RoPE and SWA\n",
        "        input_names=[\"input_ids\", \"attention_mask\"],\n",
        "        output_names=[\"last_hidden_state\"],\n",
        "        dynamic_axes={\n",
        "            \"input_ids\": {0: \"batch\", 1: \"seq\"},\n",
        "            \"attention_mask\": {0: \"batch\", 1: \"seq\"},\n",
        "        },\n",
        "        dynamo=False\n",
        "    )\n",
        "    print(\"SUCCESS: The encoder has been exported.\")\n",
        "except Exception as e:\n",
        "    print(f\"Export failed: {e}\")"
      ],
      "metadata": {
        "id": "87BA-strEA1N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}